{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Tree\n",
    "The following Jupyter Notebook shows the use of Pyspark.ML `GBTRegressor`. In the notebook, we'll first define our schema structure and populate it with our data. Our data is currently stored in `cleaning.txt`, a simple text file representation of the output our 'Data Cleaning PIG Script' produces.\n",
    "\n",
    "We'll perform some simple data transformations to ensure any categorical features are presented as DoubleType using Pyspark.ML's `StringIndexer` API. We'll do the same for any Integer typed features.\n",
    "\n",
    "We'll then use the Pyspark.ML `VectorIndexer` to produce a vector representation of our input features. This step will be added as a stage in our ML Pipeline called **VectorAssembler**. Then we'll normalise the input values so that they are all on the same scale. This normalizer step will also be added to our Pipeline as a stage named **normalizer**.\n",
    "\n",
    "Finally, we'll instantiate our GBT Model using Pyspark.ML's `GBTRegressor`. This will be the final stage in our ML Pipeline, and represents the `fit()` stage. It will simply be named **GBT**. We'll use our Pipeline to instantiate a model which will be fit on our training data. We'll then use this trained model to transform our test data and receive a salary prediction for each data point. We'll calculate a measure of success using **RMSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer, StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkML Pipeline Building\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/seanc/ca4022_assignment_3/book/data/replaced_salary_data/cleaned.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e470cdff9aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load and parse the data file, converting it to a DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/seanc/ca4022_assignment_3/book/data/replaced_salary_data/cleaned.txt"
     ]
    }
   ],
   "source": [
    "# Let's define our schema\n",
    "schema = StructType([\\\n",
    "    StructField(\"timestamp\", StringType(), True),\\\n",
    "    StructField(\"company\", StringType(), True),\\\n",
    "    StructField(\"level\", StringType(), True),\\\n",
    "    StructField(\"title\", StringType(), True),\\\n",
    "    StructField(\"totalyearlycompensation\", IntegerType(), False),\\\n",
    "    StructField(\"location\", StringType(), True),\\\n",
    "    StructField(\"yearsofexperience\", FloatType(), False),\\\n",
    "    StructField(\"yearsatcompany\", FloatType(), False),\\\n",
    "    StructField(\"tag\", StringType(), True),\\\n",
    "    StructField(\"basesalary\", IntegerType(), False),\\\n",
    "    StructField(\"stockgrantvalue\", IntegerType(), False),\\\n",
    "    StructField(\"bonus\", IntegerType(), False),\\\n",
    "    StructField(\"gender\", StringType(), True),\\\n",
    "    StructField(\"cityid\", StringType(), True),\\\n",
    "    StructField(\"dmaid\", StringType(), True),\\\n",
    "    StructField(\"race\", StringType(), True),\\\n",
    "    StructField(\"education\", StringType(), True)])\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"delimiter\", \"\\t\")\\\n",
    "    .schema(schema)\\\n",
    "    .load(\"data/replaced_salary_data/cleaned.txt\")\n",
    "data.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of Categorical Features\n",
    "In the next cell, we will list all of our categorical features currently of the `string` type. All ML models require data to presented in numerical values. We will use the `StringIndexer` method to transform our categorical features into numerical columns.\n",
    "\n",
    "This is performed as a pre-processing step to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different indexer for each categorical column\n",
    "cols_to_be_indexed = ['company', 'level', 'title', 'location', 'gender', 'race', 'education']\n",
    "\n",
    "indexed_cols = ['company_index', 'level_index', 'title_index', 'location_index',\n",
    "                'gender_index', 'race_index', 'education_index']\n",
    "\n",
    "#Let's create a copy of our data to work from\n",
    "indexed = data\n",
    "\n",
    "indexer = StringIndexer(inputCols=cols_to_be_indexed, outputCols=indexed_cols)\n",
    "indexed = indexer.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typecasting of Features\n",
    "In this step, we will convert all of our `integer` features into `double` features. Again, this is a formality, and is better practice for ML pipeline building. Like the previous step, we will transform our integer values straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of numerical columns to turn into double-type\n",
    "\n",
    "numeric_cols = ['totalyearlycompensation', 'yearsofexperience', 'yearsatcompany', 'basesalary', 'stockgrantvalue', 'bonus',\n",
    "               'cityid', 'dmaid']\n",
    "for col in numeric_cols:\n",
    "    indexed = indexed.withColumn(col, data[col].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling Input Features\n",
    "Here we assemble all of the input features for our model. This includes our previously Typecast features, and our Encoded categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_list = ['gender_index', 'race_index', 'education_index', 'company_index', 'title_index',\n",
    "                'level_index', 'location_index', 'totalyearlycompensation', 'yearsofexperience', 'yearsatcompany',\n",
    "                'basesalary', 'stockgrantvalue', 'bonus', 'cityid', 'dmaid']\n",
    "\n",
    "#This is our Vector Assembler object, it will be put into our Pipeline later on\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_list, outputCol='features', handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count for column gender_index: 5\n",
      "Distinct Count for column race_index: 6\n",
      "Distinct Count for column education_index: 6\n",
      "Distinct Count for column company_index: 1102\n",
      "Distinct Count for column title_index: 15\n",
      "Distinct Count for column level_index: 2919\n",
      "Distinct Count for column location_index: 1050\n",
      "Distinct Count for column totalyearlycompensation: 893\n",
      "Distinct Count for column yearsofexperience: 65\n",
      "Distinct Count for column yearsatcompany: 81\n",
      "Distinct Count for column basesalary: 482\n",
      "Distinct Count for column stockgrantvalue: 610\n",
      "Distinct Count for column bonus: 335\n",
      "Distinct Count for column cityid: 1045\n",
      "Distinct Count for column dmaid: 150\n"
     ]
    }
   ],
   "source": [
    "#Let's show the number of unique values per column\n",
    "for col in feature_list:\n",
    "    print(f\"Distinct Count for column {col}: \" + str(indexed.select(col).distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise Feature Set\n",
    "Our input features are not on one singular scale. This can damage the performance of our ML model. For example, `yearsofexperience` will be a small value, less than 50. Whereas `basesalary` can be upwards of 1,000,000. We require all values to be within a similar range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Test split\n",
    "In the following cell, we are simply splitting our data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62642\n",
      "Dropping na's\n",
      "62640\n"
     ]
    }
   ],
   "source": [
    "print(indexed.count())\n",
    "print(\"Dropping na's\")\n",
    "indexed = indexed.dropna()\n",
    "print(indexed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our training and test data\n",
    "splits = indexed.randomSplit([0.8, 0.2])\n",
    "trainingData = splits[0]\n",
    "testingData = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation\n",
    "When following the previous steps from **Encoding of Categorical Features**, through to **Normalise Feature Set**, you'll notice that the `inputCol` used in the function is usually the `outputCol` from the previous function. This is done purposely so that the individual Transformer pieces of our pipeline fit together.\n",
    "\n",
    "In our stages, we start with our Vector Assembler. That feeds into our Normalizer as the final transformation step. The final stage is the `fit()` stage which is the Gradient Boosted Trees Regressor model itself. We will define it in the following cells then instantiate our Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our gbt with our target column as basesalary\n",
    "gbt = GBTRegressor(labelCol='basesalary', featuresCol='features_norm', maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now we instantiate our pipeline with the stages as discussed above.\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, gbt])\n",
    "\n",
    "#We can then take this pipeline, fit it to our training data\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data\n",
    "Now that we've built our model, let's test its accuracy using the test data we set aside earlier. We can apply our model to data using `model.transform(<data>)`. This will create a DataFrame object, where each row has a new column named `prediction`. \n",
    "\n",
    "Let's take a look at the prediction column. As our dataset is quite large, we'll view a subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-----------------+------+---------------+----+----------+----------+\n",
      "|       company|            location|               level|yearsofexperience|gender|      education|race|basesalary|prediction|\n",
      "+--------------+--------------------+--------------------+-----------------+------+---------------+----+----------+----------+\n",
      "|        INTUIT|Bangalore, KA, India|               Staff|             12.0|    NA|             NA|  NA|  141907.0| 138655.57|\n",
      "|      FACEBOOK|      Menlo Park, CA|                  E7|             15.0|  Male|             NA|  NA|  260000.0| 279183.57|\n",
      "|       EXPEDIA|         Seattle, WA|                   N|             20.0|  Male|             NA|  NA|  185000.0| 184842.58|\n",
      "|        AMAZON|         Seattle, WA|               SDE I|              0.0|  Male|             NA|  NA|  108000.0| 109591.21|\n",
      "|        ORACLE|    Redwood City, CA|                  M4|             27.0|    NA|             NA|  NA|  141907.0| 139532.64|\n",
      "|          LYFT|   San Francisco, CA|                  T5|              4.0|    NA|             NA|  NA|  192000.0| 190732.75|\n",
      "|JPMORGAN CHASE|     Jersey City, NJ|                 601|              3.0|    NA|             NA|  NA|  120000.0| 117469.59|\n",
      "|        AMAZON|         Seattle, WA|                  L5|              6.0|  Male|             NA|  NA|  129000.0| 136469.25|\n",
      "|        AIRBNB|   San Francisco, CA|                  L3|              0.0|  Male|Master's Degree|  NA|  130000.0|  132011.4|\n",
      "|      LINKEDIN|   San Francisco, CA|Staff Software En...|              6.0|    NA|Master's Degree|  NA|  205000.0| 198240.34|\n",
      "+--------------+--------------------+--------------------+-----------------+------+---------------+----+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(trainingData)\n",
    "\n",
    "#Let's just quickly make our predictions rounded to 2 decimals\n",
    "prediction = prediction.withColumn(\"prediction\",round(prediction[\"prediction\"],2))\n",
    "\n",
    "prediction.select('company', 'location', 'level', 'yearsofexperience',\n",
    "                  'gender', 'education', 'race', 'basesalary', 'prediction').show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 11567.4\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"basesalary\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "In the following section, we'll be looking at using `CrossValidator` to improve the overall performance of model. We'll also be seeting up a Grid Search problem using `ParamGridBuilder` from Pyspark.ML for hyperparameter tuning. These are important steps in model development.\n",
    "\n",
    "**Important Note:** Running a Grid Search on a CPU will take considerable time to finish, and is generally not advisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Running this on a CPU is not advisable.\n",
    "\n",
    "#Let's set up our grid search\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [20, 50, 100, 200, 250]) \\\n",
    "    .addGrid(gbt.maxDepth, [0, 1, 2, 5, 10, 20 , 30]) \\\n",
    "    .addGrid(gbt.stepSize, [0.01, 0.05, 0.1]) \\\n",
    "    .addGrid(gbt.subsamplingRate, [0.05,0.1, 0.2, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "#Let's set up our cross validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps = paramGrid,\n",
    "                         evaluator = evaluator,\n",
    "                         numFolds=5)\n",
    "\n",
    "#Fit the model\n",
    "cvModel = crossval.fit(trainingData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}